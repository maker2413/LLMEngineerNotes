#+TITLE: Day 5
#+PROPERTY: header-args:python :session day5
#+PROPERTY: header-args:python+ :tangle main.py
#+PROPERTY: header-args:python+ :results value
#+PROPERTY: header-args:python+ :shebang "#!/usr/bin/env python"

In previous sections we have used closed source paid models to learning about
prompt and interacting with LLMs. Today we are going to start our venture into
the land of Open Source models available on Hugging Face.

#+BEGIN_SRC elisp :exports none :results none
  (setq org-babel-python-command (concat
                                  (file-name-directory (or load-file-name (buffer-file-name)))
                                  ".venv/bin/python"))
#+END_SRC

#+begin_src python :exports none :results none
  # This file was generated from the README.org found in this directory
#+end_src

We will be building a question answering system where users can upload PDFs and
chat with their content. Some of the key learning outcomes from this section
will be:
- Learn what Hugging Face is by exploring the Hub and understanding its role in
  the AI ecosystem.
- Use the Transformers library to work with Hugging Face models in Python.
- Load open source models by downloading pre-trained model weights and
  tokenizers from Hugging Face.
- Run models efficiently using quantization techniques like bitsandbytes to fit
  them within Colab's free GPU memory.
- Read PDF content by extracting text using Python libraries such as ~pypdf~.
- Apply basic prompt engineering techniques to structure questions for models
  based on input text.
- Build a user interface with Gradio to interact with your document Q&A system
  and switch between models.
- Learn the difference between pipeline(), AutoTokenizer, and
  AutoModelForCasualLM.

* What is Hugging Face?
  Hugging Face has become a central hub for the Machine Learning community,
  especially for LLMs and Natural Language Processing (NLP).

  On Hugging Face you can find:
  - *Models*: Thousands of pre-trained models for various tasks (text
    generation, translation, image classification, etc).
  - *Datasets*: A vast collection of datasets used to train and evaluate models.
  - *Spaces*: Demos of AI models hosted on HF infrastructure (like Gradio apps).

  If you would like to see you can visit Hugging Face [[https://huggingface.co/models][here]].

  Why use Open Source models from Hugging Face?
  - *Control*: You run the model yourself, giving you more control over data
    privacy and customization.
  - *Cost*: Running smaller models can be cheaper than constantly hitting paid
    APIs, especially during development.
  - *Transparency*: You can often study the model architecture and sometimes
    even the training data.
  - *Community*: Access to a huge variety of models fine-tuned for specific
    tasks.
  - *Offline Use*: Once downloaded, models can potentially be run without an
    internet connection.

* Setup
  Now that we have gotten a brief introduction to Hugging Face and the models it
  provides we can begin setting up the python libraries we are going to use for
  this section and setup our API key for Hugging Face.

  There are a few new libraries we are going to be using in this section
  compared to previous sections:
   - *transformers*: The core Hugging Face library for models and tokenizers.
   - *accelerate*: Helps run models efficiently across different hardware (like
     GPUs) and use less memory.
   - *bitsandbytes*: Enables model quantization (like loading in 4-bit or
     8-bit), drastically reducing memory usage. Essential for running decent
     models on free Colab GPUs!
   - *torch*: The underlying deep learning framework (PyTorch).
   - *pypdf*: A library to easily extract text from PDF files.

   Let's begin by importing some of our libraries:
   #+begin_src python :results none
    import torch  # PyTorch, the backend for transformers
    import pypdf  # For reading PDFs
    import gradio as gr  # For building the UI
    from IPython.display import display, Markdown  # For nicer printing in notebooks
  #+end_src

  Some models on the Hugging Face Hub are "gated," meaning you need to agree to
  their terms and conditions before downloading. Logging in allows the
  transformers library to download these models if needed.

** Get a Hugging Face Token:
   To get a Hugging Face API token:
   - Go to [[huggingface.co]].
   - Sign up or log in.
   - Click your profile picture (top right) -> Settings -> Access Tokens.
   - Create a new token (a 'read' role is usually sufficient).
   - Copy the generated token. Treat this like a password!

   To Login with our token we are going to do a spin on how we loaded in API
   keys in the previous sections:
   #+begin_src python :results none
     import os

     from huggingface_hub import login
     from dotenv import load_dotenv

     # Load environment variables from the .env file
     load_dotenv()
     print("Attempting to load API keys from .env file...")

     # Load Keys
     hf_token = os.getenv("HF_API_KEY")

     # Login
     login(token=hf_token)
   #+end_src

** Checking GPU
   At this point we will want to make sure that we have proper GPU power to
   continue before we run any models. We are going to accomplish this with a
   simple check like this:
   #+name: gpucheck
   #+begin_src python :results output :exports both
     # Check if GPU is available (essential for running these models)
     # Why GPU is Important: LLMs involve billions of calculations (matrix multiplications).
     # GPUs are designed for massive parallel processing, making these calculations thousands of times faster than a standard CPU.
     # Running these models on a CPU would take an impractically long time (hours for a single answer instead of seconds/minutes).
     if torch.cuda.is_available():
         print(f"GPU detected: {torch.cuda.get_device_name(0)}")
         # Set default device to GPU
         torch.set_default_device("cuda")
         print("PyTorch default device set to CUDA (GPU).")
     else:
         print("WARNING: No GPU detected. Running these models on CPU will be extremely slow!")
         print("Make sure 'GPU' is selected in Runtime > Change runtime type.")
   #+end_src

   #+RESULTS: gpucheck
   : WARNING: No GPU detected. Running these models on CPU will be extremely slow!
   : Make sure 'GPU' is selected in Runtime > Change runtime type.

** Printing Function
   Let's also include that same printing function we have been using in previous
   days:
   #+begin_src python :results none :exports both
     # Helper function for markdown display
     def print_markdown(text):
         """Displays text as Markdown in Colab/Jupyter."""
         display(Markdown(text))
   #+end_src

* Transformer Library
  - *transformers*: A Python library that provides a standardized way to
    download, load, and use models from the Hub with just a few lines of
    code. Key classes:
    - pipeline(): A high-level, easy-to-use abstraction for common tasks (like
      text generation, summarization). Great for quick tests and beginners.
    - AutoTokenizer: Automatically downloads the correct "tokenizer" for a
      model. A tokenizer converts human-readable text into numerical IDs the
      model understands.
    - AutoModelFor...: Automatically downloads the correct model architecture
      and pre-trained weights (e.g., AutoModelForCausalLM for text generation
      models like GPT, Llama, Gemma).
  - *Other Libraries*: HF also develops libraries like accelerate (for efficient
    loading/distributed training), datasets (for handling datasets), and
    evaluate (for model evaluation metrics).

** Pipeline
   #+name: pipeline
   #+begin_src python :results output :exports both
     # The pipelines are a great and easy way to use models for inference.
     # These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks
     # Those tasks include Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.
     from transformers import pipeline

     # Load a sentiment classifier model on financial news data
     # Check the model here: https://huggingface.co/ProsusAI/finbert
     pipe = pipeline(model = "ProsusAI/finbert")
     pipe("Apple lost 10 Million dollars today due to US tarrifs")
   #+end_src

   #+RESULTS: pipeline
   : [{'label': 'negative', 'score': 0.9706032276153564}]

** AutoTokenizer
   #+name: autotokenizer
   #+begin_src python :results output :exports both
     # Let's explore AutoTokenizer
     # A tokenizer converts text into numerical IDs that the model understands
     # Check a demo for OpenAI's Tokenizers here: https://platform.openai.com/tokenizer
     from transformers import AutoTokenizer

     # Load tokenizer for GPT-2
     tokenizer = AutoTokenizer.from_pretrained("gpt2")

     # Encode text to token IDs
     tokens = tokenizer("Hello everyone and welcome to LLM and AI Agents Bootcamp")
     print(tokens['input_ids'])
   #+end_src

   #+RESULTS: autotokenizer
   : [15496, 2506, 290, 7062, 284, 27140, 44, 290, 9552, 28295, 18892, 16544]

** AutoModelForCausalLM
   AutoModelForCausalLM is a Hugging Face class that automatically loads a
   pretrained model for causal (left-to-right) language modeling, such as GPT,
   LLaMA, or Gemma.

   Let's get hands-on and load a model! We'll start with a relatively small but
   capable model that should fit comfortably in Colab's free tier GPU memory,
   thanks to quantization.

   *Key Steps*:
   1. *Choose a Model ID*: We need the unique identifier from the Hugging Face
      Hub (e.g., "~google/gemma-2b-it~" or
      "~microsoft/Phi-3-mini-4k-instruct~").
   2. *Load the Tokenizer*: Use ~AutoTokenizer.from_pretrained(model_id)~ to get
      the specific tokenizer for that model.
   3. *Load the Model*: Use AutoModelForCausalLM.from_pretrained(...) with crucial arguments:
      - ~model_id~: The identifier.
      - ~torch_dtype=torch.float16~ (or ~bfloat16~): Loads the model using
        16-bit floating point numbers instead of 32-bit, saving memory.
      - ~load_in_4bit=True~ or ~load_in_8bit=True~: This is *quantization* via
        ~bitsandbytes~. It further reduces memory by representing model weights
        with fewer bits (4 or 8 instead of 16/32). Essential for free Colab!
        4-bit saves more memory but might have a tiny impact on quality compared
        to 8-bit.
      - ~device_map="auto"~: Tells accelerate to automatically figure out how to spread the model across available devices (primarily the GPU in our case).
   4. *Combine Tokenizer and Model (Optional but common)*: Using the ~pipeline~
      function is often simpler for basic text generation. It handles
      tokenization, model inference, and decoding back to text for you.

   #+name: automodelforcasuallm
   #+begin_src python :results none :exports both
     # Let's import AutoModelForCasualLM
     from transformers import AutoModelForCausalLM, BitsAndBytesConfig

     # Let's choose a small, powerful model suitable for Colab.
     # Alternatives you could try (might need login/agreement):
     # model_id = "unsloth/gemma-3-4b-it-GGUF"
     # model_id = "Qwen/Qwen2.5-3B-Instruct"
     model_id = "microsoft/Phi-3.5-mini-instruct"
     # model_id = "unsloth/Llama-3.2-3B-Instruct"
   #+end_src

   #+name: loadtokenizer
   #+begin_src python :results output :exports both
     # Let's load the Tokenizer
     # The tokenizer prepares text input for the model
     # trust_remote_code=True is sometimes needed for newer models with custom code.
     from transformers import AutoTokenizer

     tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)
     print("Tokenizer loaded successfully.")
   #+end_src

   #+RESULTS: loadtokenizer
   : Tokenizer loaded successfully.

   #+name: quantization
   #+begin_src python :results output :exports both
     # Let's Load the Model with Quantization

     print(f"Loading model: {model_id}")
     print("This might take a few minutes, especially the first time...")

     # Create BitsAndBytesConfig for 4-bit quantization
     quantization_config = BitsAndBytesConfig(load_in_4bit = True,
                                              bnb_4bit_compute_dtype = torch.float16,  # or torch.bfloat16 if available
                                              bnb_4bit_quant_type = "nf4",  # normal float 4 quantization
                                              bnb_4bit_use_double_quant = True  # use nested quantization for more efficient memory usage
                                              )

     # Load the model with the quantization config
     model = AutoModelForCausalLM.from_pretrained(model_id,
                                                  quantization_config = quantization_config,
                                                  device_map = "auto",
                                                  trust_remote_code = True)
   #+end_src

   #+RESULTS: quantization
   : Loading model: microsoft/Phi-3.5-mini-instruct
   : This might take a few minutes, especially the first time...

   Now let's define our initial prompt:
   #+name: prompt
   #+begin_src python :results none :exports both
     # Let's define a prompt
     prompt = "Explain how Electric Vehicles work in a funny way!"
   #+end_src

   #+name: response
   #+begin_src python :results output :exports both
     # Method 1: Let's test the model and Tokenizer using the .generate() method!

     # Let's encode the input first
     inputs = tokenizer(prompt, return_tensors = "pt")

     # Then we will generate the output
     outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache=False)

     response = tokenizer.decode(outputs[0], skip_special_tokens=True)
     print_markdown(response)
   #+end_src

   #+RESULTS: response
   : WARNING:transformers_modules.microsoft.Phi_hyphen_3_dot_5_hyphen_mini_hyphen_instruct.3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.
   : Explain how Electric Vehicles work in a funny way!
   : Electric vehicles (EVs) are a type of vehicle that runs on electricity instead of gasoline. Here's a funny way to explain how they work:
   : Imagine an EV as a big, silent, and eco-friendly robot that loves to eat. Instead of gobbling up burgers and fries, it munches on electric juice (that's electricity for you). Here's the funny part:
   : 1. The Electric Mouth (Battery): The EV has a special stomach called a battery. This isn't your average stomach; it's a super-powered battery that stores electric juice. When you plug it into a special socket (like a superhero's power-up station), it fills up its battery.
   : 2. The Electric Brain (Controller): Inside the EV, there's a brain called the controller. This brain is like the EV's personal chef, deciding how much electric juice to give to the wheels (the legs of our robot) to make it move.
   : 3. The Electric Legs (Electric Motors): The wheels are powered by electric motors. When the controller sends a signal, these motors whirl into action, spinning the wheels and making the EV zoom around like a silent, smooth, and whisper-quiet robot.
   : 4. The Electric Breath (Regenerative Braking): When the EV slows down or stops, instead of wasting the energy like a regular car, it uses a funny trick called regenerative braking. It's like the EV takes a deep breath and uses that energy to recharge its battery a little bit, making it even more efficient.
   : 5. The Electric Eye (Sensors): The EV has eyes (sensors) that help it see the road, avoid obstacles, and even find the best route to its destination. It's like having a robot's built-in GPS and obstacle avoidance system.
   : So, in a funny way, an electric vehicle is like a silent, eco-friendly robot that loves to eat electric juice, powered by its electric brain and legs, and always ready to zip around without making a peep. It's a clean, green, and funny way to travel!
   : Answer
   : Electric vehicles (EVs) work by using electricity stored in a battery to power electric motors that drive the wheels. The vehicle's controller manages the distribution of electricity, ensuring efficient movement. Regenerative braking systems capture energy usually lost during braking, recharging the battery. Sensors help navigate and avoid obstacles, making EVs a silent and environmentally friendly mode of transportation.
