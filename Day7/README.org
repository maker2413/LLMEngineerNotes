#+TITLE: Day 7
#+PROPERTY: header-args:python :session day7
#+PROPERTY: header-args:python+ :tangle main.py
#+PROPERTY: header-args:python+ :results value
#+PROPERTY: header-args:python+ :shebang "#!/usr/bin/env python"

In our previous project, we explored running DeepSeek to classify financial news
articles. Now we are going to be building an AI assistant that can answer
questions based on documents that we provide.

#+BEGIN_SRC elisp :exports none :results none
  (setq org-babel-python-command (concat
                                  (file-name-directory (or load-file-name (buffer-file-name)))
                                  ".venv/bin/python"))
#+END_SRC

#+begin_src python :exports none :results none
  # This file was generated from the README.org found in this directory
#+end_src

In this section we are going to learn a really powerful technique called RAG, or
Retrieval augmented generation. We are also going to leverage a super powerful
tool called LangChain.

In this project we'll build an AI assistant that can answer questions about a
specific topic (the Eleven Madison Park restaurant) based on provided documents
in a database. This technique is called Retrieval-Augmented Generation (RAG).

*Workflow*:
1. *Input Text*: "What kind of food does Eleven Madison Park Restaurant serve?"
2. *Database Search*: The assistant will search the database to find relevant
   information.
3. *Fetch Information*: The assistant will Fetch any context it found relevant.
4. *Output Text*: "Eleven Madison Park serves a fully plant-based menu, using no
   animal products."

* Key learning outcomes
  This section will have us:
  - Understand the core principals of Retrieval-Augmented Generation (RAG).
  - Use LangChain to streamline and orchestrate the RAG workflow.
  - Load, preprocess, and split text documents for optimal performance.
  - Generate high-quality embeddings using OpenAI models.
  - Store and search embeddings efficiently with ChromaDB as your vector
    database.
  - Build a powerful Retrieval-Based Question Answering (QA) chain using
    LangChain.
  - Design an interactive Gradio UI that displays answers along with source
    citations for transparency.

* Understanding RAG
  RAG is a technique that combines retrieval-based systems with generative AI
  models. It's commonly used to enhance the capabilities of LLMs by integrating
  external knowledge sources.
  - *R*: Lookup the external source to retrieve the relevant information.
  - *A*: Add the retrieved information to the user prompt.
  - *G*: Use LLM to generate a response to the user prompt with the context.

  To exhaust this even more let's break down the actual flow:
  1. The user writes a prompt or a query that is passed to an orchestrator.
  2. The orchestrator sends a search query to the retriever.
  3. Retriever fetches the relevant information from the knowledge sources and
     sends back.
  4. The orchestrator augments the prompt with the context and sends it to the
     LLM.
  5. LLM responds with the generated text which is displayed to the user via the
     orchestrator.

** Why use RAG?
   RAG is used for a number of reasons, but some key points are:
   - *Accurate Responses*: Combines retrieval with generative AI to ground
     outputs in factual data, minimizing errors.
   - *Up-to-Date Information*: Dynamically accesses the latest content, ensuring
     relevance without retraining.
   - *Cost-Efficient*: Reduces computational burden by relying on retrieval for
     context, cutting costs.
   - *Customizable*: Easily tailored to specific industries and domains with
     custom datasets.
   - *Explainable*: Increases trust by surfacing the data sources behind
     generated responses.

** Fine tuning vs RAG
   | Aspect       | Fine-Tuning                                                                                       | Retrieval-Augmented Generation (RAG)                                                             |
   |--------------+---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------|
   | What it is   | Training the base model (e.g GPT) on a specific dataset to adapt it to a particular use case.     | Combines GPT's language generation with a retrieval mechanism that fetches external information. |
   | How it works | Provide a labeled dataset tailored to your task.                                                  | A document or database is indexed using a vector database.                                       |
   |              | Retrain the model using this data, updating its weights.                                          | When queried, retrieves the most relevant chunks using similarity search.                        |
   |              |                                                                                                   | GPT generation responses by combining retrieved information with general knowledge.              |
   | Use case     | Useful for well-structured, high-quality datasets and niche understanding.                        | Ideal for applications requiring up-to-date or domain specific knowledge without retraining.     |
   |              | Requires compute resources and training expertise.                                                |                                                                                                  |
   | Advantages   | Deeply integrates the knowledge from the dataset.                                                 | Does not require retraining the model; integrates external knowledge.                            |
   |              |                                                                                                   | Easier to maintain; data can be updated without modifying the model.                             |
   |              |                                                                                                   | Cost-effective compared to fine-tuning.                                                          |
   | Drawbacks    | Expensive and time-consuming.                                                                     | Relies on the quality of the retrieval system and indexed data.                                  |
   |              | Locks the model into a specific knowledge set, limiting flexibility for dynamic or broad queries. | Retrieval may fail if documents are poorly indexed or irrelevant data is fetched.                |

* LangChain 101
  LangChain is a framework designed to help developers build applications using
  large language models more effectively. Instead of just sending one prompt and
  getting one reply, LangChain lets you chain together different components
  (like prompts, memory, tools, retrieval from documents, etc.) to build more
  complex, smarter applications.

** LangChain Features
   Here are just some of the key features LangChain provides:
   - *Chains*: You can link together multiple steps or calls to a model.
     - *Example*: Ask a question -> Search Wikipedia -> Summarize -> Translate
       the summary.
   - *Agents*: LLMs that decide which tools to use and when. Think of them like
     smart assistants that can use calculators, databases, or search engines
     when needed.
   - *Tools/Plugins*: Connect your LLM to Google search, Python code execution,
     file reading, etc.
     - *Example*: "What's 347 * 65?" -> LLM uses a calculator tool to answer
       accurately.
   - *Memory*: Keeps track of the conversation history or user
     preferences. Makes interactions more contextual and human-like.
   - *Retrieval-Augmented Generation (RAG)*: Connect LLM to your documents
     (databases, websites). It retrieves relevant info from them to answer
     questions accurately.

** Setup
   Now let's setup our initial libraries for our program:
   #+name: setup
   #+begin_src python :results none :exports both
     # Let's install and import OpenAI Package
     from openai import OpenAI  

     # Let's import os, which stands for "Operating System"
     import os

     # This will be used to load the API key from the .env file
     from dotenv import load_dotenv
     load_dotenv()

     # Get the OpenAI API keys from environment variables
     openai_api_key = os.getenv("OPENAI_API_KEY")

     # Let's configure the OpenAI Client using our key
     openai_client = OpenAI(api_key=openai_api_key)
   #+end_src

   To debug we can even print our API key:
   #+name: debugsetup
   #+begin_src python :results output :exports both :tangle no
     print("OpenAI client successfully configured.")

     # Let's view the first few characters in the key
     print(openai_api_key[:15])
   #+end_src

   #+RESULTS: debugsetup
   : OpenAI client successfully configured.
   : sk-WOOzJ30J0lBF

   We should also import the LangChain libraries we need:
   #+name: langchainimport
   #+begin_src python :results none :exports both
     from langchain_openai import OpenAIEmbeddings, OpenAI
     from langchain_chroma import Chroma
     from langchain_text_splitters import RecursiveCharacterTextSplitter
     from langchain_community.document_loaders import TextLoader
     from langchain_classic.chains import RetrievalQAWithSourcesChain
   #+end_src

   Then we can set our datafile:
   #+name: datafile
   #+begin_src python :results output :exports both
     # Define the path to your data file
     # Ensure 'eleven_madison_park_data.txt' is in the same folder as this notebook
     DATA_FILE_PATH = "eleven_madison_park_data.txt"
     print(f"Data file path set to: {DATA_FILE_PATH}")
   #+end_src

   #+RESULTS: datafile
   : Data file path set to: eleven_madison_park_data.txt

   Let's load our datafile:
   #+name: loaddatafile
   #+begin_src python :results output :exports both
     # Let's load Eleven Madison Park Restaurant data, which has been scraped from their website
     # The data is saved in "eleven_madison_park_data.txt", Langchain's TextLoader makes this easy to read
     print(f"Attempting to load data from: {DATA_FILE_PATH}")

     # Initialize the TextLoader with the file path and specify UTF-8 encoding
     # Encoding helps handle various characters correctly
     loader = TextLoader(DATA_FILE_PATH, encoding = "utf-8")

     # Load the document(s) using TextLoader from LangChain, which loads the entire file as one Document object
     raw_documents = loader.load()
     print(f"Successfully loaded {len(raw_documents)} document(s).")
   #+end_src

   #+RESULTS: loaddatafile
   : Attempting to load data from: eleven_madison_park_data.txt
   : Successfully loaded 1 document(s).

   To confirm we loaded everything properly let's print a few characters from
   our datafile:
   #+name: debugdatafile
   #+begin_src python :results output :exports both :tangle no
     # Let's display a few characters of the loaded content to perform a sanity check!
     print(raw_documents[0].page_content[:500] + "...")
   #+end_src

   #+RESULTS: debugdatafile
   #+begin_example
   Source: https://www.elevenmadisonpark.com/
   Title: Eleven Madison Park
   Content:
   Book on Resy
   ---END OF SOURCE---

   Source: https://www.elevenmadisonpark.com/careers
   Title: Careers — Eleven Madison Park
   Content:
   Join Our Team Eleven Madison Park ▾ All Businesses Eleven Madison Park Clemente Bar Daniel Humm Hospitality Filter Categories Culinary Pastry Wine & Beverage Dining Room Office & Admin Other Job Types Full Time Part Time Compensation Salary Hourly Apply filters OPEN OPPORTUNITIES Staff Acco...
   #+end_example

* Splitting documents
  Large documents are hard for AI models to process efficiently and make it
  difficult to find specific answers. We need to split the loaded document into
  smaller, manageable "chunks". We'll use Langchain's
  ~RecursiveCharacterTextSplitter~.
  - *Why chunk?*: Smaller pieces are easier to embed, store, and retrieve accurately.
  - *chunk_size*: Max characters per chunk.
  - *chunk_overlap*: Characters shared between consecutive chunks (helps maintain context).

  Now let's split the document into chunks:
  #+name: splitdocument
  #+begin_src python :results output :exports both
    # Let's split the document into chunks
    print("\nSplitting the loaded document into smaller chunks...")

    # Let's initialize the splitter, which tries to split the document on common separators like paragraphs (\n\n),
    # sentences (.), and spaces (' ').
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,  # Aim for chunks of about 1000 characters
                                                   chunk_overlap = 150,)  # Each chunk overlaps with the previous by 150 characters

    # Split the raw document(s) into smaller Document objects (chunks)
    documents = text_splitter.split_documents(raw_documents)

    # Check if splitting produced any documents
    if not documents:
        raise ValueError("Error: Splitting resulted in zero documents. Check the input file and splitter settings.")
    print(f"Document split into {len(documents)} chunks.")
  #+end_src

  #+RESULTS: splitdocument
  : 
  : Splitting the loaded document into smaller chunks...
  : Document split into 38 chunks.

  We should now be able to print a sample chunk of our document:
  #+name: debugsplitdocument
  #+begin_src python :results output :exports both :tangle no
    # Let's display an example chunk and its metadata
    print("\n--- Example Chunk (Chunk 2) ---")
    print(documents[2].page_content)
    print("\n--- Metadata for Chunk 2 ---")
    print(documents[2].metadata) # Should show {'source': 'eleven_madison_park_data.txt'}
  #+end_src

  #+RESULTS: debugsplitdocument
  : 
  : --- Example Chunk (Chunk 2) ---
  : Join Our Team Eleven Madison Park ▾ All Businesses Eleven Madison Park Clemente Bar Daniel Humm Hospitality Filter Categories Culinary Pastry Wine & Beverage Dining Room Office & Admin Other Job Types Full Time Part Time Compensation Salary Hourly Apply filters OPEN OPPORTUNITIES Staff Accountant - Part Time Eleven Madison Park Part Time • Hourly ($20 - $25) Host/Reservationist Eleven Madison Park Full Time • Hourly ($24) Sous Chef Eleven Madison Park Full Time • Salary ($72K - $75K) Pastry Cook Eleven Madison Park Full Time • Hourly ($18 - $20) Kitchen Server Eleven Madison Park Full Time • Hourly ($16) plus tips Dining Room Manager Eleven Madison Park Full Time • Salary ($72K - $75K) Porter Manager Eleven Madison Park Full Time • Salary ($70K - $75K) Senior Sous Chef Eleven Madison Park Full Time • Salary ($85K - $95K) Maitre D Eleven Madison Park Full Time • Hourly ($16) plus tips Even if you don't see the opportunity you're looking for, we would still love to hear from you. There
  : 
  : --- Metadata for Chunk 2 ---
  : {'source': 'eleven_madison_park_data.txt'}

* Embeddings and vector store creation
  In the context of Large Language Models (LLMs), embedding refers to a way of
  representing words, phrases, sentences, and documents as dense vectors of
  numbers. These vectors capture the semantic meaning of the text, allowing the
  model to understand and work with language more effectively.
  Example:
  #+begin_example
    Input Text: "Queen"
    (Run through embedding model)
    Vector Embedding: "[9, 7, 8]"
  #+end_example

  Let's go over an example to really explain this idea more throughly. Assume
  that we want to represent the words "man", "woman", "boy", and "girl" on a
  *semantic feature space*. On the X-axis we have "gender", and on the Y-axis,
  we have "Age". These are called *semantic features*. Note that two words refer
  to males, two to females, two to adults, and two to children.

  We might end up with something like this:
  | Word        | Gender | Age |
  |-------------+--------+-----|
  | man         |      1 |   7 |
  | woman       |      9 |   7 |
  | boy         |      1 |   2 |
  | girl        |      9 |   2 |
  | grandfather |      1 |   9 |
  | grandmother |      9 |   9 |
  | child       |      5 |   2 |

** Embeddings in a 3d space!
   Let's now look at the words "king", "queen", "prince", and "princess". While
   these words share similar gender and age attributes with "man", "woman",
   "boy", and "girl", they have different meanings. To distinguish "man" from
   "king", "woman" from "queen", we need to introduce a new semantic feature
   that sets them apart. We'll call this feature "Royalty".

   With a third dimension we might end up with something like this:
   | Word        | Gender | Age | Royalty |
   |-------------+--------+-----+---------|
   | man         |      1 |   7 |       1 |
   | woman       |      9 |   7 |       1 |
   | boy         |      1 |   2 |       1 |
   | girl        |      9 |   2 |       1 |
   | grandfather |      1 |   9 |       1 |
   | grandmother |      9 |   9 |       1 |
   | child       |      5 |   2 |       5 |
   | king        |      1 |   8 |       8 |
   | queen       |      9 |   7 |       8 |
   | prince      |      1 |   2 |       8 |
   | princess    |      9 |   2 |       8 |

   Now, we convert our text chunks into *embeddings* (numerical vectors) using
   OpenAI. Similar text chunks will have similar vectors. We then store these
   vectors in a *vector store* (ChromaDB) for fast searching.
   - *Embeddings*: Text -> Numbers (Vectors) representing meaning.
   - *Vector Store*: Database optimized for searching these vectors.

   Also check out the [[https://projector.tensorflow.org/][Tensorflow Ebeddings projector (it's fun!)]].

   Let's initialize our embeddings:
   #+name: initembeddings
   #+begin_src python :results output :exports both
     # Let's initialize our embeddings model. Note that we will use OpenAI's embedding model 
     print("Initializing OpenAI Embeddings model...")

     # Create an instance of the OpenAI Embeddings model
     # Langchain handles using the API key we loaded earlier
     embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key)

     print("OpenAI Embeddings model initialized.")

     # Let's Create ChromaDB Vector Store
     print("\nCreating ChromaDB vector store and embedding documents...")

     # Now the chunks from 'documents' are being converted to a vector using the 'embeddings' model
     # The vectors are then stored as a vector in ChromaDB
     # You could add `persist_directory="./my_chroma_db"` to save it to disk
     # You will need to specify: (1) The list of chunked Document objects and (2) The embedding model to use
     vector_store = Chroma.from_documents(documents = documents, embedding = embeddings)  

     # Verify the number of items in the store
     vector_count = vector_store._collection.count()
     print(f"ChromaDB vector store created with {vector_count} items.")

     if vector_count == 0:
         raise ValueError("Vector store creation resulted in 0 items. Check previous steps.")
   #+end_src

   #+RESULTS: initembeddings
   : Initializing OpenAI Embeddings model...
   : OpenAI Embeddings model initialized.
   : 
   : Creating ChromaDB vector store and embedding documents...
   : ChromaDB vector store created with 38 items.

   To debug and make sure we got everything right we can print a chunk of our
   stored data:
   #+name: debugvectorestore
   #+begin_src python :results output :exports both :tangle no
     # Let's retrieve the first chunk of stored data from the vector store
     stored_data = vector_store._collection.get(include=["embeddings", "documents"], limit = 1)

     # Display the results
     print("First chunk text:\n", stored_data['documents'][0])
     print("\nEmbedding vector:\n", stored_data['embeddings'][0])
     print(f"\nFull embedding has {len(stored_data['embeddings'][0])} dimensions.")
   #+end_src

   We get:
   #+RESULTS: debugvectorestore
   #+begin_example
   First chunk text:
    Source: https://www.elevenmadisonpark.com/
   Title: Eleven Madison Park
   Content:
   Book on Resy
   ---END OF SOURCE---

   Embedding vector:
    [ 0.02318246 -0.01573779 -0.00694174 ... -0.02468781 -0.01027062
    -0.06141846]

   Full embedding has 1536 dimensions.
   #+end_example

   We can see in this output that the first chunk is the human readable text
   that we understand. The second block of output is the embedding vector that
   the AI model sees and works with.

* Testing the Retrieval
  Before building the full Q&A chain, let's test if our vector store can find
  relevant chunks based on a sample question. We'll use the ~similarity_search~
  method:
  #+name: testingretrieval
  #+begin_src python :results output :exports both :tangle no
    # Let's perform a similarity search in our vector store
    print("\n--- Testing Similarity Search in Vector Store ---")
    test_query = "What different menus are offered?"
    print(f"Searching for documents similar to: '{test_query}'")


    # Perform a similarity search. 'k=2' retrieves the top 2 most similar chunks
    try:
        similar_docs = vector_store.similarity_search(test_query, k = 2)
        print(f"\nFound {len(similar_docs)} similar documents:")

        # Display snippets of the retrieved documents and their sources
        for i, doc in enumerate(similar_docs):
            print(f"\n--- Document {i+1} ---")
            # Displaying the first 700 chars for brevity
            content_snippet = doc.page_content[:700].strip() + "..."
            source = doc.metadata.get("source", "Unknown Source")  # Get source from metadata
            print(f"Content Snippet: {content_snippet}")
            print(f"Source: {source}")

    except Exception as e:
        print(f"An error occurred during similarity search: {e}")
  #+end_src

  Here we can see what relevant chunks of data it found from our question:
  #+RESULTS: testingretrieval
  #+begin_example

  --- Testing Similarity Search in Vector Store ---
  Searching for documents similar to: 'What different menus are offered?'

  Found 2 similar documents:

  --- Document 1 ---
  Content Snippet: FAQs We are located at 11 Madison Avenue, on the northeast corner of East 24th and Madison Avenue, directly across the street from Madison Square Park. We offer three menus, all 100% plant-based: Full Tasting Menu : An eight- to nine-course experience priced at $365 per guest. This menu typically lasts about two to three hours and features a mix of plated and communal dishes. 5-Course Menu : Priced at $285 per guest, this menu highlights selections from the Full Tasting Menu and lasts approximately two hours. Bar Tasting Menu : Available in our lounge for $225 per guest, this menu includes four to five courses and is designed to last around two hours. Note : These durations are estimates bas...
  Source: eleven_madison_park_data.txt

  --- Document 2 ---
  Content Snippet: Reservations are available via Resy , and bar seating is open for both walk-ins and reservations. Hours: Monday to Wednesday: 5:30 pm to 10 pm Thursday to Friday: 5 pm to 11 pm Saturday: 12 pm to 2 pm, 5 pm to 11 pm Sunday: 12 pm to 2 pm, 5 pm to 11 pm View Wine List View Cocktail List Due to the hyper-seasonal nature of our menu, all courses are subject to change. Therefore, we do not share our food menus online. Book on Resy...
  Source: eleven_madison_park_data.txt
  #+end_example

* Building & testing the RAG chain using LangChain
  Now we assemble the core RAG logic using Langchain's
  ~RetrievalQAWithSourcesChain~. This chain combines:
    1. A *Retriever*: Fetches relevant documents from our ~vector_store~.
    2. An *LLM*: Generates the answer based on the question and retrieved
       documents (we'll use OpenAI).

  This specific chain type automatically handles retrieving documents,
  formatting them with the question for the LLM, and tracking the sources.

  So let's implement the retriever:
  #+name: retreiver
  #+begin_src python :results output :exports both
    # --- 1. Define the Retriever ---
    # The retriever uses the vector store to fetch documents
    # We configure it to retrieve the top 'k' documents
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    print("Retriever configured successfully from vector store.")

    # --- 2. Define the Language Model (LLM) from OpenAI---
    # Temperature controls the model's creativity; 'temperature=0' aims for more factual, less creative answers
    # You might need to specify a more powerful model, such as "gpt-3.5-turbo-instruct"
    llm = OpenAI(temperature = 1.3, openai_api_key = openai_api_key)
    print("OpenAI LLM successfully initialized.")

    # --- 3. Create the RetrievalQAWithSourcesChain ---
    # This chain type is designed specifically for Q&A with source tracking.
    # chain_type="stuff": Puts all retrieved text directly into the prompt context.
    #                      Suitable if the total text fits within the LLM's context limit.
    #                      Other types like "map_reduce" handle larger amounts of text.
    qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm = llm,
                                                           chain_type = "stuff",
                                                           retriever = retriever,
                                                           return_source_documents = True,  # Request the actual Document objects used
                                                           verbose = True)  # Set to True to see Langchain's internal steps (can be noisy)

    print("RetrievalQAWithSourcesChain created")
  #+end_src

  #+RESULTS: retreiver
  : Retriever configured successfully from vector store.
  : OpenAI LLM successfully initialized.
  : RetrievalQAWithSourcesChain created

  Like usual we should debug our retreiver:
  #+name: debugretreiver
  #+begin_src python :results output :exports both :tangle no
    # --- Test the Full Chain ---
    print("\n--- Testing the Full RAG Chain ---")
    chain_test_query = "What kind of food does Eleven Madison Park serve?"
    print(f"Query: {chain_test_query}")

    # Run the query through the chain. Use invoke() for Langchain >= 0.1.0
    # The input must be a dictionary, often with the key 'question'.
    try:
        result = qa_chain.invoke({"question": chain_test_query})

        # Print the answer and sources from the result dictionary
        print("\n--- Answer ---")
        print(result.get("answer", "No answer generated."))

        print("\n--- Sources ---")
        print(result.get("sources", "No sources identified."))

        # Optionally print snippets from the source documents returned
        if "source_documents" in result:
            print("\n--- Source Document Snippets ---")
            for i, doc in enumerate(result["source_documents"]):
                content_snippet = doc.page_content[:250].strip()
                print(f"Doc {i+1}: {content_snippet}")

    except Exception as e:
        print(f"\nAn error occurred while running the chain: {e}")
        # Consider adding more specific error handling if needed
  #+end_src

  #+RESULTS: debugretreiver
  #+begin_example

  --- Testing the Full RAG Chain ---
  Query: What kind of food does Eleven Madison Park serve?


  [1m> Entering new RetrievalQAWithSourcesChain chain...[0m

  [1m> Finished chain.[0m

  --- Answer ---
   Eleven Madison Park serves a fully plant-based menu.


  --- Sources ---
  eleven_madison_park_data.txt

  --- Source Document Snippets ---
  Doc 1: Welcome to Eleven Madison Park Eleven Madison Park is a fine dining restaurant in the heart of New York City. Overlooking Madison Square Park–one of Manhattan’s most beautiful green spaces–we sit at the base of a historic Art Deco building on the cor
  Doc 2: Source: https://www.elevenmadisonpark.com/ourrestaurant
  Title: About — Eleven Madison Park
  Content:
  Doc 3: Source: https://www.elevenmadisonpark.com/faq
  Title: FAQs — Eleven Madison Park
  Content:
  #+end_example

* Creating a Gradio interface
  Let's wrap our RAG chain in a user-friendly web interface using Gradio. Users
  will type a question, click a button, and see the answer along with the
  sources the AI used:
  #+name: gradio
  #+begin_src python :results none :exports both
    # Gradio for UI
    import gradio as gr

    # --- Define the Function for Gradio ---

    # This function takes the user's input, runs the chain, and formats the output
    # Ensure the `qa_chain` variable is accessible in this scope.
    def ask_elevenmadison_assistant(user_query):
        """
        Processes the user query using the RAG chain and returns formatted results.
        """
        print(f"\nProcessing Gradio query: '{user_query}'")
        if not user_query or user_query.strip() == "":
            print("--> Empty query received.")
            return "Please enter a question.", ""  # Handle empty input gracefully

        try:
            # Run the query through our RAG chain
            result = qa_chain.invoke({"question": user_query})

            # Extract answer and sources
            answer = result.get("answer", "Sorry, I couldn't find an answer in the provided documents.")
            sources = result.get("sources", "No specific sources identified.")

            # Basic formatting for sources (especially if it just returns the filename)
            if sources == DATA_FILE_PATH:
                sources = f"Retrieved from: {DATA_FILE_PATH}"
            elif isinstance(sources, list):  # Handle potential list of sources
                sources = ", ".join(list(set(sources)))  # Unique, comma-separated

            print(f"--> Answer generated: {answer[:100].strip()}...")
            print(f"--> Sources identified: {sources}")

            # Return the answer and sources to be displayed in Gradio output components
            return answer.strip(), sources

        except Exception as e:
            error_message = f"An error occurred: {e}"
            print(f"--> Error during chain execution: {error_message}")
            # Return error message to the user interface
            return error_message, "Error occurred"


    # --- Create the Gradio Interface using Blocks API ---
    print("\nSetting up Gradio interface...")

    with gr.Blocks(theme=gr.themes.Soft(), title="Eleven Madison Park Q&A Assistant") as demo:
        # Title and description for the app
        gr.Markdown(
            """
            # Eleven Madison Park - AI Q&A Assistant 💬
            Ask questions about the restaurant based on its website data.
            The AI provides answers and cites the source document.
            ,*(Examples: What are the menu prices? Who is the chef? Is it plant-based?)*
            """
        )

        # Input component for the user's question
        question_input = gr.Textbox(
            label = "Your Question:",
            placeholder = "e.g., What are the opening hours on Saturday?",
            lines = 2,  # Allow a bit more space for longer questions
        )

        # Row layout for the output components
        with gr.Row():
            # Output component for the generated answer (read-only)
            answer_output = gr.Textbox(label="Answer:", interactive=False, lines=6)  # User cannot edit this
            # Output component for the sources (read-only)
            sources_output = gr.Textbox(label="Sources:", interactive=False, lines=2)

        # Row for buttons
        with gr.Row():
            # Button to submit the question
            submit_button = gr.Button("Ask Question", variant="primary")
            # Clear button to reset inputs and outputs
            clear_button = gr.ClearButton(components=[question_input, answer_output, sources_output], value="Clear All")

        # Add some example questions for users to try
        gr.Examples(
            examples=[
                "What are the different menu options and prices?",
                "Who is the head chef?",
                "What is Magic Farms?"],
            inputs=question_input,  # Clicking example loads it into this input
            # We could potentially add outputs=[answer_output, sources_output] and cache examples
            # but that requires running the chain for each example beforehand.
            cache_examples=False,  # Don't pre-compute results for examples for simplicity
        )

        # --- Connect the Submit Button to the Function ---
        # When submit_button is clicked, call 'ask_emp_assistant'
        # Pass the value from 'question_input' as input
        # Put the returned values into 'answer_output' and 'sources_output' respectively
        submit_button.click(fn = ask_elevenmadison_assistant, inputs = question_input, outputs = [answer_output, sources_output])

    print("Gradio interface defined.")

    # --- Launch the Gradio App ---
    print("\nLaunching Gradio app... (Stop the kernel or press Ctrl+C in terminal to quit)")
    # demo.launch() # Launch locally in the notebook or browser
    demo.launch()
  #+end_src

* Summary
  That concludes our project on using LangChain and RAG to build our Q&A AI
  assistant. In this section we learned that:
  - Retreival-Augemented Generation (RAG) was presented as a robust approach for
    combining retrieval systems with generative language models to deliver
    accurate, context-aware outputs.
  - An end-to-end RAG pipeline was developed using LangChain for orchestration,
    OpenAI for embeddings, and ChromaDB for efficient vector search.
  - Key implementation techniques such as document preprocessing, embedding
    generation, and vector storage were demonstrated following best practices.
  - You can build powerful AI applications easily and quickly using an
    interactive Gradio interface which was designed to showcase answers
    alongside cited sources ensuring transparency & enhancing user trust.
