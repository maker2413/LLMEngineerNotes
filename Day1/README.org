#+TITLE: Day 1
#+PROPERTY: header-args:python :session day1
#+PROPERTY: header-args:python+ :tangle main.py
#+PROPERTY: header-args:python+ :results value
#+PROPERTY: header-args:python+ :shebang "#!/usr/bin/env python"

In this section we are going to build an Character AI Chatbot using OpenAI's
API.

#+BEGIN_SRC elisp :exports none :results none
  (setq org-babel-python-command (concat
                                  (file-name-directory (or load-file-name (buffer-file-name)))
                                  ".venv/bin/python"))
#+END_SRC

#+begin_src python :results none
  from openai import OpenAI
#+end_src

#+name: apikey
#+begin_src python :results output :exports both
  # Let's import "os" module, which stands for "Operating System"
  # The os module in Python provides a way to interact with the operating system for things like:
  # (1) accessing Environment Variables
  # (2) Creating, renaming, and deleting files/folders.
  import os

  # This will be used to load the API key from the .env file
  from dotenv import load_dotenv
  load_dotenv()

  # Get the OpenAI API keys from environment variables
  openai_api_key = os.getenv("OPENAI_API_KEY")

  # Let's configure the OpenAI Client using our key
  openai_client = OpenAI(api_key = openai_api_key)
  print("OpenAI client successfully configured.")

  # Let's view the first few characters in the key
  print("Key begins with:", openai_api_key[:5])
#+end_src

#+RESULTS: apikey
: OpenAI client successfully configured.
: Key begins with: sk-WO

Let's send our very first message to the OpenAI API and get a reply! This is
like saying "Hello" for the first time.

We'll use the ~chat.completions.create~ method. Think of it as the function
specifically designed for conversations.

*Key Ingredients*:
- model: Which AI brain (model) to use? We'll start with "gpt-4o-mini" - it's
  relatively cheap & Smart.
- messages: This is a list of messages in the conversation so far. Each message
  has:
  - role: Who is speaking? "user" (you) or "assistant" (the AI). Later we'll add
    "system".
  - content: What was actually said? (The text of the message).

#+name: message
#+begin_src python :results output :exports both
  # Let's define the message we want to send as the 'user'
  my_message = "Write a Poem to my mom Laila congratulating her for her 74th birthday!"
  print(f"Sending message to OpenAI: '{my_message}'")
#+end_src

#+RESULTS: message
: Sending message to OpenAI: 'Write a Poem to my mom Laila congratulating her for her 74th birthday!'

#+name: firstapicall
#+begin_src python :results none :exports both
  # Let's make an API call to OpenAI and send our message
  response = openai_client.chat.completions.create(model = "gpt-4o-mini",
                                                   messages = [{"role": "user", "content": my_message}])
#+end_src

#+name: firstreply
#+begin_src python :results output :exports both
  # Let's obtain the AI's reply from the response object
  # The response contains lots of info; we need to dig into it to find the text.
  # It usually looks like: response -> choices -> [first choice] -> message -> content
  ai_reply_content = response.choices[0].message.content

  # Let's print the reply
  print("\nðŸ¤– AI's Reply: \n")
  print(f"{ai_reply_content}")
#+end_src

#+RESULTS: firstreply
#+begin_example

ðŸ¤– AI's Reply: 

,**To Laila, on Your 74th Birthday**

In a garden where memories blossom bright,  
Today we gather, our hearts full of light.  
Seventy-four years of laughter and grace,  
A journey abundant, no challenge you face.  

With wisdom like rivers, your kindness flows wide,  
A beacon of love, our familyâ€™s guide.  
Through seasons of joy and moments of strife,  
You've woven the fabric of this beautiful life.  

From kitchen aromas that warm up the soul,  
To stories that comfort and make us feel whole,  
Each shared memory, a thread in the weave,  
Of love that unites, in which we believe.  

The twinkle in your eye, the spark in your heart,  
A tapestry woven, each thread plays its part.  
For all of the lessons, the laughter, the care,  
In this beautiful journey, weâ€™re grateful youâ€™re here.  

So hereâ€™s to you, Mom, on this milestone day,  
With love everlasting, in every way.  
May this year be filled with joy that won't cease,  
Happy birthday, dear Laila, may your heart be at peace.
#+end_example

* PRACTICE OPPORTUNITY:
  Now it's your turn to experiment with OpenAI API; perform the following tasks:
  Change the text inside the my_message variable. Ask a different question, like
  "What is the tallest mountain in the world?" or "Explain how electric vehicles
  work in a funny way." See how the AI responds! Try a different AI model,
  change the model from model="gpt-4o-mini" to model="gpt-4o"

  #+name: practice1
  #+begin_src python :results output :exports both
    my_message = "What is the tallest mountain in the world?"
    response = openai_client.chat.completions.create(model = "gpt-4o",
                                                     messages = [{"role": "user", "content": my_message}])
    ai_reply_content = response.choices[0].message.content

    # Print the reply
    print("\nðŸ¤– AI's Reply: \n")
    print(f"{ai_reply_content}")
  #+end_src

  #+RESULTS: practice1
  : 
  : ðŸ¤– AI's Reply: 
  : 
  : The tallest mountain in the world is Mount Everest, which is part of the Himalayas on the border between Nepal and the Tibet Autonomous Region of China. Its peak reaches an elevation of 8,848.86 meters (29,031.7 feet).

* Understanding the response structure
  #+name: responsestructure
  #+begin_src python :tangle no :exports both
    response
  #+end_src

  #+RESULTS: responsestructure
  : ChatCompletion(id='chatcmpl-Bw6rK87eaUiEAuVPVn4WOq1T6nBXZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The tallest mountain in the world is Mount Everest, which is part of the Himalayas on the border between Nepal and the Tibet Autonomous Region of China. Its peak reaches an elevation of 8,848.86 meters (29,031.7 feet).', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753189090, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_07871e2ad8', usage=CompletionUsage(completion_tokens=51, prompt_tokens=16, total_tokens=67, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))

  Let's explore the metadata of a response generated by OpenAI's API. Here's a
  breakdown of the key information:
  - *Model Used*: gpt-4o-mini-2024-07-18 A lightweight version of GPT-4 Omni
    designed for faster and more efficient responses.
  - *Response ID*: 'chatcmpl-BVOdqrzvuwTjmZk1hUomTknzbqLkO' A unique identifier
    for this specific completion.
  - *Role*: 'assistant' Indicates the response was generated by the AI.
  - *Finish Reason*: 'stop' The model stopped generating output naturally (not
    due to errors or max tokens).
  - *Created Timestamp*: 1746822590 Unix time format for when the response was
    created.
  - *Prompt Tokens*: Number of tokens in the input prompt 26.
  - *Completion Tokens*: Number of tokens generated in the response 256.
  - *Total Tokens*: Combined count of prompt + completion tokens 282.
  - *Audio, Function Calls, Tool Calls, Annotations*: None The response did not
    include any of these features.
  - *Refusal*: None The model did not refuse the task.
  - *System Fingerprint & Service Tier*: Internal metadata used for system
    tracking and optimization.
